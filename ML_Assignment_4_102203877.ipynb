{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMAoe2raffy/FsBojgYppke"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Q1 (Based on Step-by-Step Implementation of Ridge Regression using Gradient\n","Descent Optimization)**"],"metadata":{"id":"PXm74y5MsH_K"}},{"cell_type":"markdown","source":["Generate a dataset with atleast seven highly correlated columns and a target variable.Implement Ridge Regression using Gradient Descent Optimization. Take different values of learning rate (such as 0.0001,0.001,0.01,0.1,1,10) and regularization parameter (10^(-15),10^(-10),10^(-5),10^(-3),0,1,10,20). Choose the best parameters for which ridge regression cost function is minimum and R2_score is maximum."],"metadata":{"id":"_kzlkX6XsOjX"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.datasets import make_regression\n","from sklearn.preprocessing import StandardScaler"],"metadata":{"id":"IWp8LfhVsmWW","executionInfo":{"status":"ok","timestamp":1725082741465,"user_tz":-330,"elapsed":843,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["np.random.seed(42)\n","n_samples=1000\n","n_features=7"],"metadata":{"id":"lcjpghKovgbz","executionInfo":{"status":"ok","timestamp":1725082742426,"user_tz":-330,"elapsed":5,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["X, y = make_regression(n_samples=n_samples, n_features=n_features, noise=0.1)"],"metadata":{"id":"sgGo27mpvqrd","executionInfo":{"status":"ok","timestamp":1725082742426,"user_tz":-330,"elapsed":4,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["X = np.dot(X, np.random.rand(n_features, n_features))"],"metadata":{"id":"RvplltxYvw6k","executionInfo":{"status":"ok","timestamp":1725082742426,"user_tz":-330,"elapsed":4,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)"],"metadata":{"id":"5YbanvnLv5W0","executionInfo":{"status":"ok","timestamp":1725082742426,"user_tz":-330,"elapsed":4,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["def ridge_regression(X, y, learning_rate, reg_param, num_iterations):\n","    m, n = X.shape\n","    theta = np.zeros(n)\n","    cost_history = []\n","\n","    for _ in range(num_iterations):\n","        predictions = X.dot(theta)\n","        errors = predictions - y\n","        gradient = (X.T.dot(errors) + reg_param * theta) / m\n","        theta -= learning_rate * gradient\n","\n","        cost = (1 / (2 * m)) * (errors.T.dot(errors) + reg_param * np.sum(theta**2))\n","        cost_history.append(cost)\n","\n","    return theta, cost_history\n"],"metadata":{"id":"xKM_Uh91wGZv","executionInfo":{"status":"ok","timestamp":1725082742427,"user_tz":-330,"elapsed":4,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n","regularization_params = [1e-15, 1e-10, 1e-5, 3, 10, 20]\n","num_iterations = 1000"],"metadata":{"id":"qjdDC4qWwN-M","executionInfo":{"status":"ok","timestamp":1725082742427,"user_tz":-330,"elapsed":4,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["best_theta = None\n","best_cost = float('inf')\n","best_r2_score = float('-inf')\n","\n","for lr in learning_rates:\n","    for rp in regularization_params:\n","        theta, cost_history = ridge_regression(X_scaled, y, lr, rp, num_iterations)\n","        r2_score = 1 - np.sum((X_scaled.dot(theta) - y) ** 2) / np.sum((y - np.mean(y)) ** 2)\n","        if cost_history[-1] < best_cost and r2_score > best_r2_score:\n","            best_theta = theta\n","            best_cost = cost_history[-1]\n","            best_r2_score = r2_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WWyl0arRwdta","executionInfo":{"status":"ok","timestamp":1725082744858,"user_tz":-330,"elapsed":2435,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}},"outputId":"e465399a-47cc-48a7-e935-eb33f4846805"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-49-7b691bcccbd6>:12: RuntimeWarning: overflow encountered in square\n","  cost = (1 / (2 * m)) * (errors.T.dot(errors) + reg_param * np.sum(theta**2))\n","/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n","  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n","<ipython-input-49-7b691bcccbd6>:12: RuntimeWarning: overflow encountered in scalar multiply\n","  cost = (1 / (2 * m)) * (errors.T.dot(errors) + reg_param * np.sum(theta**2))\n"]}]},{"cell_type":"code","source":["print(f'Best parameters: Learning rate = {lr}, Regularization parameter = {rp}')\n","print(f'Best cost: {best_cost}')\n","print(f'Best R2 score: {best_r2_score}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O3AZFGpCwuc3","executionInfo":{"status":"ok","timestamp":1725082744859,"user_tz":-330,"elapsed":6,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}},"outputId":"6513986a-5012-4d82-8cd3-561caa4b6fc7"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Best parameters: Learning rate = 10, Regularization parameter = 20\n","Best cost: 36.312743654535716\n","Best R2 score: 0.9948759669787856\n"]}]},{"cell_type":"markdown","source":["Q2) Load the Hitters dataset from the following link\n","https://drive.google.com/file/d/1qzCKF6JKKMB0p7ul_lLy8tdmRk3vE_bG/view?usp=sharing"],"metadata":{"id":"YLR7r1GPw_iQ"}},{"cell_type":"markdown","source":["(a) Pre-process the data (null values, noise, categorical to numerical encoding)"],"metadata":{"id":"4JYxRKpUxDcG"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer"],"metadata":{"id":"ZTQ9f_CHxIAx","executionInfo":{"status":"ok","timestamp":1725082744859,"user_tz":-330,"elapsed":3,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["hitters=pd.read_csv('/content/Hitters.csv')"],"metadata":{"id":"DcZeFJU30p8O","executionInfo":{"status":"ok","timestamp":1725082745796,"user_tz":-330,"elapsed":940,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["hitters.dropna(inplace=True)"],"metadata":{"id":"jHUp0r-10goL","executionInfo":{"status":"ok","timestamp":1725082745796,"user_tz":-330,"elapsed":10,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["X = hitters.drop('Salary', axis=1)\n","y = hitters['Salary']"],"metadata":{"id":"GI1PaI351ceV","executionInfo":{"status":"ok","timestamp":1725082745796,"user_tz":-330,"elapsed":10,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["categorical_cols = X.select_dtypes(include=['object']).columns"],"metadata":{"id":"nsRN22wX1uev","executionInfo":{"status":"ok","timestamp":1725082745796,"user_tz":-330,"elapsed":9,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', StandardScaler(), [col for col in X.columns if col not in categorical_cols]),\n","        ('cat', OneHotEncoder(), categorical_cols)\n","    ])"],"metadata":{"id":"LwBxpvuF1zo5","executionInfo":{"status":"ok","timestamp":1725082745796,"user_tz":-330,"elapsed":9,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["preprocessor.fit(X)\n","X_preprocessed = preprocessor.transform(X)"],"metadata":{"id":"eIoLNtLO19od","executionInfo":{"status":"ok","timestamp":1725082745796,"user_tz":-330,"elapsed":9,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["print(\"Pre-processing completed. Data is ready for model fitting.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mld_ryqO2CqG","executionInfo":{"status":"ok","timestamp":1725082745796,"user_tz":-330,"elapsed":8,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}},"outputId":"933529c4-e003-4bde-a7a2-2c600f8a1b91"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["Pre-processing completed. Data is ready for model fitting.\n"]}]},{"cell_type":"markdown","source":["(b) Separate input and output features and perform scaling"],"metadata":{"id":"vuER4AIr2PUt"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"metadata":{"id":"pdSbHd9w2VlQ","executionInfo":{"status":"ok","timestamp":1725082745796,"user_tz":-330,"elapsed":7,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)"],"metadata":{"id":"pAEulBB42nRK","executionInfo":{"status":"ok","timestamp":1725082745796,"user_tz":-330,"elapsed":6,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["print(\"Data has been split into training and testing sets.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TpYNId772rM1","executionInfo":{"status":"ok","timestamp":1725082745796,"user_tz":-330,"elapsed":6,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}},"outputId":"ae884b33-52e3-463a-c368-0fae7b3f9f71"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Data has been split into training and testing sets.\n"]}]},{"cell_type":"markdown","source":["(c) Fit a Linear, Ridge (use regularization parameter as 0.5748), and LASSO (use\n","regularization parameter as 0.5748) regression function on the dataset."],"metadata":{"id":"ZqH-RMJq23wg"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression, Ridge, Lasso\n","from sklearn.metrics import mean_squared_error"],"metadata":{"id":"bLqQArI625B0","executionInfo":{"status":"ok","timestamp":1725082745796,"user_tz":-330,"elapsed":4,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["models = {\n","    'Linear Regression': LinearRegression(),\n","    'Ridge Regression': Ridge(alpha=0.5748),\n","    'Lasso Regression': Lasso(alpha=0.5748)\n","}"],"metadata":{"id":"zxzvob8o3HR3","executionInfo":{"status":"ok","timestamp":1725082745797,"user_tz":-330,"elapsed":5,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["for name, model in models.items():\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    mse = mean_squared_error(y_test, y_pred)\n","    print(f'{name} Mean Squared Error: {mse}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TiV-LzQR3RK6","executionInfo":{"status":"ok","timestamp":1725082745797,"user_tz":-330,"elapsed":5,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}},"outputId":"5ecf26e3-92db-42ba-8d08-6aee9bd1d9b7"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Linear Regression Mean Squared Error: 128284.34549672344\n","Ridge Regression Mean Squared Error: 126606.39854037874\n","Lasso Regression Mean Squared Error: 126543.07184906653\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.253e+04, tolerance: 4.367e+03\n","  model = cd_fast.enet_coordinate_descent(\n"]}]},{"cell_type":"markdown","source":["(d) Evaluate the performance of each trained model on test set. Which model performs\n","the best and Why?"],"metadata":{"id":"zRqDAHVU3lA7"}},{"cell_type":"code","source":[],"metadata":{"id":"8-qIZt0p4AdU","executionInfo":{"status":"ok","timestamp":1725082746559,"user_tz":-330,"elapsed":2,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":["**Q 3 Cross Validation for Ridge and Lasso Regression**"],"metadata":{"id":"oAJSEUxZ4VVB"}},{"cell_type":"markdown","source":["Explore Ridge Cross Validation (RidgeCV) and Lasso Cross Validation (LassoCV)\n","function of Python. Implement both on Boston House Prediction Dataset (load_boston\n","dataset from sklearn.datasets)."],"metadata":{"id":"FnhhnebR4bwX"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import RidgeCV, LassoCV\n","from sklearn.metrics import mean_squared_error"],"metadata":{"id":"XTd9khPI4XoB","executionInfo":{"status":"ok","timestamp":1725082748686,"user_tz":-330,"elapsed":3,"user":{"displayName":"Diya Burman","userId":"13165710535503864528"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OubHJPWf5izY"},"execution_count":null,"outputs":[]}]}